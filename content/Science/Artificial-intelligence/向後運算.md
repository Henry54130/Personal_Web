---
title:
  - 神經網路的向後運算
---

## 1️⃣ 神經網路基本架構

- **輸入**：$x_1, x_2$  
- **加權求和**：  
$$
z = w_1 x_1 + w_2 x_2 ...+ b
$$
- **激活函數**：  
$$
\hat{y} = \sigma(z)
$$
- **真實標籤**：$y$  

---

## 2️⃣ 損失函數 (Loss)

常用 **均方誤差 (MSE)**：

$$
L = \frac{1}{2}  (y - \hat{y})^2
$$

> 目標：讓 $L$ 最小。

---

## 3️⃣ 反向傳播核心：鏈式法則

每個權重的梯度：

$$
\frac{\partial L}{\partial w_i} =
\frac{\partial L}{\partial \hat{y}} \cdot
\frac{\partial \hat{y}}{\partial z} \cdot
\frac{\partial z}{\partial w_i}
$$


---

## 4️⃣ 每一項展開

1. **Loss 對輸出 $\hat{y}$ 的導數**：

$$
\frac{\partial L}{\partial \hat{y}} = \hat{y} - y
$$

2. **輸出對加權輸入 $z$ 的導數**（假如使用sigmoid為 $\sigma(z)$）： [[sigmoid_微分.md|sigmoid 微分]]

$$
\frac{\partial \hat{y}}{\partial z} = \sigma'(z) = \hat{y}(1-\hat{y})
$$

3. **加權輸入對權重 $w_i$ 的導數**：

$$
\frac{\partial z}{\partial w_i} = x_i
$$

---

## 5️⃣ 梯度合併(計算每個參數對 Loss 的影響（梯度）)

$$
\frac{\partial L}{\partial w_i} = (\hat{y} - y) \cdot \hat{y}(1-\hat{y}) \cdot x_i
$$

---

## 6️⃣ 參數更新（梯度下降法）(沿梯度反方向更新參數 → Loss 下降)

$$
w_i^{(t+1)} = w_i^{(t)} - \alpha \frac{\partial L}{\partial w_i}
$$

$$
b^{(t+1)} = b^{(t)} - \alpha \frac{\partial L}{\partial b}
$$

> 根據 Loss 的梯度，往 **下降方向** 調整 $w$ 和 $b$，直到 Loss 收斂。

formula from this
<img src="向後計算部分圖示.jpeg.png" alt="向後計算部分圖示.jpeg.png">

---

## 7️⃣ 梯度消失問題

- Sigmoid 對極大或極小 $z$ 導數 $\sigma'(z) \approx 0$  
- 導致梯度幾乎消失 → 權重無法更新 → 神經元「死亡」  
- 解決方法：使用 **ReLU** 等激活函數避免梯度消失

---

### 🔑 核心總結

> NN 的逆向運算就是：  
> 利用 **鏈式法則**從 Loss 開始，一層層把梯度傳回，算出每個權重與偏差對 Loss 的影響，然後用 **梯度下降法**更新參數。

---
link:
back to [[0_unmanaged/MD/向前運算.md|向前運算]]
